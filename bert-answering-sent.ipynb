{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dùng sentence bert cho bài toán searching dùng embedding (Question Answering).\n",
    "\n",
    "## Các bước thực hiện:\n",
    "\n",
    "1. Load data từ database, dùng underthesea để tách câu.\n",
    "2. Dùng sentence bert (bi-encoder) để tạo embedding cho toàn bộ câu bóc tách. \n",
    "3. Compare embedding của câu hỏi với embedding của các câu bóc tách, lấy ra top k câu có embedding gần nhất.\n",
    "4. Dùng cross encoder để lấy ra 3 câu match nhất với câu hỏi.\n",
    "5. Load ra 3 câu sau mỗi câu match đúng nhất.\n",
    "\n",
    "- Dùng code từ file:\n",
    "\n",
    "https://github.com/UKPLab/sentence-transformers/blob/master/examples/applications/retrieve_rerank/retrieve_rerank_simple_wikipedia.ipynb\n",
    "\n",
    "- Document để hiểu hơn về bài toán:\n",
    "https://aclanthology.org/2022.emnlp-industry.16.pdf\n",
    "\n",
    "\n",
    "![Bi encoder and cross encoder](images/bi-encoder.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/g6/37kt02914kx36yzcbbqfyck00000gn/T/ipykernel_38623/3062833682.py:6: TqdmExperimentalWarning: Using `tqdm.autonotebook.tqdm` in notebook mode. Use `tqdm.tqdm` instead to force console mode (e.g. in jupyter console)\n",
      "  from tqdm.autonotebook import tqdm\n"
     ]
    }
   ],
   "source": [
    "# We also compare the results to lexical search (keyword search). Here, we use \n",
    "# the BM25 algorithm which is implemented in the rank_bm25 package.\n",
    "\n",
    "from sklearn.feature_extraction import _stop_words\n",
    "import string\n",
    "from tqdm.autonotebook import tqdm\n",
    "import numpy as np\n",
    "import json\n",
    "from sentence_transformers import SentenceTransformer, CrossEncoder, util\n",
    "import gzip\n",
    "import os\n",
    "import torch\n",
    "import pickle\n",
    "import pandas as pd\n",
    "\n",
    "from importlib import reload\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/g6/37kt02914kx36yzcbbqfyck00000gn/T/ipykernel_38623/866573376.py:16: UserWarning: pandas only supports SQLAlchemy connectable (engine/connection) or database string URI or sqlite3 DBAPI2 connection. Other DBAPI2 objects are not tested. Please consider using SQLAlchemy.\n",
      "  df_ques = pd.read_sql(query, connection)\n",
      "/var/folders/g6/37kt02914kx36yzcbbqfyck00000gn/T/ipykernel_38623/866573376.py:19: UserWarning: pandas only supports SQLAlchemy connectable (engine/connection) or database string URI or sqlite3 DBAPI2 connection. Other DBAPI2 objects are not tested. Please consider using SQLAlchemy.\n",
      "  df_answer = pd.read_sql(query_ans, connection)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['javascript']\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
    "import mysql.connector\n",
    "import json\n",
    "\n",
    "# Establish a connection to the MySQL database\n",
    "connection = mysql.connector.connect(\n",
    "    host='127.0.0.1',\n",
    "    port=13306,\n",
    "    user='root',\n",
    "    password='root',\n",
    "    database='pyml'\n",
    ")\n",
    "\n",
    "# Read the table data using pandas\n",
    "query = \"\"\"SELECT vi.hash_id, vi.contents as question_content, JSON_UNQUOTE(JSON_EXTRACT(tags, '$[*].slug')) AS slug FROM viblo_interview vi\"\"\"\n",
    "df_ques = pd.read_sql(query, connection)\n",
    "\n",
    "query_ans = \"\"\"SELECT va.hash_id, va.contents, va.question_id FROM viblo_answer va\"\"\"\n",
    "df_answer = pd.read_sql(query_ans, connection)\n",
    "\n",
    "# Close the database connection\n",
    "connection.close()\n",
    "\n",
    "\n",
    "def filter_slug(x):\n",
    "    if x is None:\n",
    "        return ''\n",
    "    t = str(x).replace(\"b'\", \"\").replace(\"'\", \"\")\n",
    "    t1 = json.loads(t)\n",
    "\n",
    "    return t1\n",
    "df_ques['slug_filter'] = df_ques['slug'].apply(filter_slug)\n",
    "print(df_ques['slug_filter'][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This function will search all wikipedia articles for passages that\n",
    "# answer the query\n",
    "\n",
    "bi_encoder = SentenceTransformer('multi-qa-MiniLM-L6-cos-v1')\n",
    "bi_encoder.max_seq_length = 256     #Truncate long passages to 256 tokens\n",
    "top_k = 32                          #Number of passages we want to retrieve with the bi-encoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#The bi-encoder will retrieve 100 documents. We use a cross-encoder, to re-rank the results list to improve the quality\n",
    "cross_encoder = CrossEncoder('cross-encoder/ms-marco-MiniLM-L-6-v2')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "from underthesea import sent_tokenize\n",
    "\n",
    "def us_split_sent(text):\n",
    "    sents = sent_tokenize(text)\n",
    "    try:\n",
    "        return sents\n",
    "    except TypeError:\n",
    "        return []\n",
    "\n",
    "\n",
    "def clear_text_vi(posts = []):\n",
    "    def split_sentences(text):\n",
    "        try:\n",
    "            sents = us_split_sent(text)\n",
    "        except Exception as e:\n",
    "            print('Error:', e, text)\n",
    "            sents = []\n",
    "        sents_1 = [sent for sent in sents if sent.strip() != \"\"]\n",
    "        return sents_1\n",
    "\n",
    "    t = map(split_sentences, posts)\n",
    "    return list(t)\n",
    "\n",
    "def flatten_list(l):\n",
    "    return [item for sublist in l for item in sublist]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_list = df_ques['question_content'].values.tolist()\n",
    "sent = clear_text_vi(data_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3617 [['Thế nào là abstract syntax tree?', 'So với cú pháp Balan thì notation nào sẽ có hiệu năng tốt hơn'], ['Nếu các kỹ thuật thường được sử dụng để trích xuất Credentials trên Windows?']]\n"
     ]
    }
   ],
   "source": [
    "print(len(sent), sent[:2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Thế nào là abstract syntax tree?', 'So với cú pháp Balan thì notation nào sẽ có hiệu năng tốt hơn']\n"
     ]
    }
   ],
   "source": [
    "sent_flat = [item for sent1 in sent for item in sent1]\n",
    "\n",
    "print(sent_flat[:2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "edcd87a221d44c10b8751f00d1d922f1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/127 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4059\n"
     ]
    }
   ],
   "source": [
    "corpus_embeddings = bi_encoder.encode(sent_flat, convert_to_tensor=True, show_progress_bar=True)\n",
    "print(len(corpus_embeddings))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Store sentences & embeddings on disc\n",
    "with open('./data/sbas-embeddings.pkl', \"wb\") as fOut:\n",
    "    pickle.dump({'sentences': sent_flat, 'embeddings': corpus_embeddings}, fOut, protocol=pickle.HIGHEST_PROTOCOL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Load sentences & embeddings from disc\n",
    "with open('./data/sbas-embeddings.pkl', \"rb\") as fIn:\n",
    "    stored_data = pickle.load(fIn)\n",
    "    sent_flat = stored_data['sentences']\n",
    "    corpus_embeddings = stored_data['embeddings']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_sent_seq_from_hit(hit, num_after = 3):\n",
    "    sent_seq = []\n",
    "    for i in range(num_after):\n",
    "        # check index exist in sent_flat before append\n",
    "        if (hit['corpus_id'] + i) < len(sent_flat):\n",
    "            sent_seq.append(sent_flat[hit['corpus_id'] + i])\n",
    "    return ' '.join(sent_seq)\n",
    "\n",
    "def search(query):\n",
    "    print(\"Input question:\", query)\n",
    "    ##### Sematic Search #####\n",
    "    query = clear_text_vi([query])\n",
    "    # print(\"Input question clean sentences:\", query)\n",
    "    query = flatten_list(query)\n",
    "    # print(\"Input question flattern:\", query)\n",
    "\n",
    "    # Encode the query using the bi-encoder and find potentially relevant passages\n",
    "    question_embedding = bi_encoder.encode(query, convert_to_tensor=True)\n",
    "    # question_embedding = question_embedding\n",
    "    hits = util.semantic_search(question_embedding, corpus_embeddings, top_k=top_k)\n",
    "    hits = hits[0]  # Get the hits for the first query\n",
    "    \n",
    "    ##### Re-Ranking #####\n",
    "    # Now, score all retrieved passages with the cross_encoder\n",
    "    cross_inp = [[query[0], sent_flat[hit['corpus_id']]] for hit in hits]\n",
    "    \n",
    "    cross_scores = cross_encoder.predict(cross_inp)\n",
    "\n",
    "    # Sort results by the cross-encoder scores\n",
    "    for idx in range(len(cross_scores)):\n",
    "        hits[idx]['cross-score'] = cross_scores[idx]\n",
    "\n",
    "\n",
    "    # Output of top-5 hits from re-ranker\n",
    "    print(\"\\n-------------------------\\n\")\n",
    "    print(\"Top-3 Cross-Encoder Re-ranker hits\")\n",
    "    hits = sorted(hits, key=lambda x: x['cross-score'], reverse=True)\n",
    "    for hit in hits[0:3]:\n",
    "        print(\"Score:\", hit['cross-score'])\n",
    "        print(\"\\t \" + get_sent_seq_from_hit(hit).replace(\"\\n\", \" \"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input question: Tình hình chứng khoán thế giới trong tháng 9 ?\n",
      "\n",
      "-------------------------\n",
      "\n",
      "Top-3 Cross-Encoder Re-ranker hits\n",
      "Score: 6.6487575\n",
      "\t Tính từ khi thị trường chứng khoán Việt Nam được thành lập từ tháng 7-2000, trong tháng 5, chỉ số VN- Index có 12 lần giảm, 9 lần tăng. Trong đó, các năm có tháng 5 giảm mạnh nhất là năm 2008 (giảm 21,7%), 2006 (giảm 12,6%), 2011 (giảm 10,5%), 2012 (giảm 9,2%). Ngược lại, các năm có tháng 5 tăng mạnh nhất là năm 2009 (tăng 26,7%), 2001 (tăng 25,6%), 2007 và 2020 (cùng tăng 15,2%) và 2013 (tăng 9,3%).\n",
      "Score: 6.6434627\n",
      "\t Chứng khoán sẽ nối dài đà tăng trong tháng 9?Các chuyên gia nhận định, thị trường chứng khoán (TTCK) vẫn duy trì được đà tăng trong những tháng tới nhờ yếu tố lãi suất cũng như kỳ vọng về kết quả kinh doanh của các doanh nghiệp sẽ tốt hơn trong thời gian tới. Duy trì đà tăng Ông Nguyễn Thế Minh - Giám đốc phân tích khối khách hàng cá nhân CTCK Yuanta Việt Nam nhận định TTCK vẫn có đủ catalyst (yếu tố xúc tác) để thúc đẩy thị trường tiếp tục đi lên. Theo đó, thị trường vừa có nhịp giảm mạnh nhất (nửa cuối tháng 8) từ tháng 5 cho đến nay, nhưng vẫn giữ được mốc hỗ trợ là 1,160 điểm.\n",
      "Score: 6.2471952\n",
      "\t >>> * Chứng khoán sẽ nối dài đà tăng trong tháng 9?. Các chuyên gia nhận định, thị trường chứng khoán (TTCK) vẫn duy trì được đà tăng trong những tháng tới nhờ yếu tố lãi suất cũng như kỳ vọng về kết quả kinh doanh của các doanh nghiệp sẽ tốt hơn trong thời gian tới. >>> * Nhu cầu tài chính, ông Đoàn Văn Hiểu Em muốn bán bớt cổ phiếu MWG.\n"
     ]
    }
   ],
   "source": [
    "search(query = \"Tình hình chứng khoán thế giới trong tháng 9 ?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input question: Giá kim loại, vàng miếng trong tháng 9, năm 2023 ?\n",
      "\n",
      "-------------------------\n",
      "\n",
      "Top-3 Cross-Encoder Re-ranker hits\n",
      "Score: 7.6517096\n",
      "         Giá vàng miếng SJC tăng 150.000 đồng mỗi lượng vào sáng 30.8. Công ty vàng bạc đá quý Sài Gòn - SJC mua vào lên 67,7 triệu đồng, bán ra 68,3 triệu đồng; Eximbank mua vào với giá 67,8 triệu đồng, bán ra 68,2 triệu đồng… So với đầu tháng 8, kim loại quý đã tăng 1,1 triệu đồng mỗi lượng.\n",
      "Score: 6.6588287\n",
      "         Giá vàng miếng SJC sáng ngày 13.6 giảm 300.000 đồng mỗi lượng, Eximbank mua vào với giá 68,2 triệu đồng/lượng và bán ra 69,1 triệu đồng/lượng; Công ty vàng bạc đá quý Sài Gòn – SJC mua vào với giá 68,45 triệu đồng/lượng và bán ra 69,35 triệu đồng/lượng… Tốc độ giảm giá của vàng trong nước nhanh hơn quốc tế khiến SJC cao hơn thế giới còn 17,15 triệu đồng/lượng.\n",
      "Score: 5.7673564\n",
      "\t So với giá kim loại quý thế giới, vàng miếng SJC cao hơn 11,7 triệu đồng/lượng, còn nữ trang và nhẫn cao hơn 2,2 - 3,3 triệu đồng/lượng. Giá vàng thế giới sáng 14.1 ít thay đổi so với mức giá chiều qua khi ở 1.822,5 USD/ounce. Trong phiên giao dịch Mỹ (đêm 13.1), giá kim loại quý đã lao dốc từ mức 1.825 USD/ounce xuống 1.810 USD/ounce nhưng sau đó dần hồi phục lên lại mức 1.822 USD/ounce.\n"
     ]
    }
   ],
   "source": [
    "search(query = \"Giá kim loại, vàng miếng trong tháng 9, năm 2023 ?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input question: Kinh tế Trung Quốc trong quí 3 năm 2023 ?\n",
      "\n",
      "-------------------------\n",
      "\n",
      "Top-3 Cross-Encoder Re-ranker hits\n",
      "Score: 7.5959907\n",
      "                     // kiem tra xem co la anh khong? == 'div') { năm 2023. Thị trường chứng khoán Mỹ giảm điểm, chỉ số Dow Jones rớt 391,76 điểm (tương đương 1,14%) xuống 33.910,85 điểm; chỉ số S&P 500 mất 0,2% còn 3.990,97 điểm, còn chỉ số Nasdaq Composite giảm 0,14% xuống 11.095,11 điểm. //Chèn ads giữa bài\n",
      "Score: 7.553033\n",
      "\t Tại Hội nghị Công tác kinh tế Trung ương thường niên hôm 16/9, các nhà lãnh Trung Quốc đã vạch ra đường hướng cho phát triển kinh tế trong năm 2023. Theo đó, năm tới, nước này sẽ tập trung vào bình ổn nền kinh tế và tiến tới điều chỉnh chính sách để đảm bảo đạt được các mục tiêu chính của nền kinh tế. “Có thể mất ít nhất một quý nữa mọi thứ tại Trung Quốc mới bắt đầu khởi sắc”, ông Dan Wang, nhà kinh tế trưởng tại ngân hàng Hang Seng Bank China, nhận định.\n",
      "Score: 7.316267\n",
      "\t Citi trước đó dự báo kinh tế Trung Quốc tăng trưởng 5,7% trong năm 2023. Dù các dữ liệu kinh tế gần đây cho thấy hoạt động sản xuất tại Trung Quốc đã phục hồi nhanh chóng, nhưng một số chỉ số khác lại cho thấy những thách thức sâu sắc về mặt hệ thống. Doanh thu bất động sản so với năm trước vẫn đang giảm, dù tốc độ giảm ít nghiêm trọng hơn so với thời điểm cuối năm 2022.\n"
     ]
    }
   ],
   "source": [
    "search(query = \"Kinh tế Trung Quốc trong quí 3 năm 2023 ?\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.3"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
