{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Use word2vec (if-idf for centroid) and random forest to tagging the question.\n",
    "\n",
    "[Ref: https://www.kaggle.com/code/antonsruberts/sentence-embeddings-centorid-method-vs-doc2vec]\n",
    "\n",
    "(It not effective, but it's a good practice for me to use word2vec and random forest)\n",
    "\n",
    "```\n",
    "Hamming Loss:  0.01\n",
    "F1 Score:  0.03015075376884422\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/g6/37kt02914kx36yzcbbqfyck00000gn/T/ipykernel_1577/1993668918.py:16: UserWarning: pandas only supports SQLAlchemy connectable (engine/connection) or database string URI or sqlite3 DBAPI2 connection. Other DBAPI2 objects are not tested. Please consider using SQLAlchemy.\n",
      "  df_ques = pd.read_sql(query, connection)\n",
      "/var/folders/g6/37kt02914kx36yzcbbqfyck00000gn/T/ipykernel_1577/1993668918.py:19: UserWarning: pandas only supports SQLAlchemy connectable (engine/connection) or database string URI or sqlite3 DBAPI2 connection. Other DBAPI2 objects are not tested. Please consider using SQLAlchemy.\n",
      "  df_answer = pd.read_sql(query_ans, connection)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['javascript']\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
    "import mysql.connector\n",
    "import json\n",
    "\n",
    "# Establish a connection to the MySQL database\n",
    "connection = mysql.connector.connect(\n",
    "    host='127.0.0.1',\n",
    "    port=13306,\n",
    "    user='root',\n",
    "    password='root',\n",
    "    database='pyml'\n",
    ")\n",
    "\n",
    "# Read the table data using pandas\n",
    "query = \"\"\"SELECT vi.hash_id, vi.contents as question_content, JSON_UNQUOTE(JSON_EXTRACT(tags, '$[*].slug')) AS slug FROM viblo_interview vi\"\"\"\n",
    "df_ques = pd.read_sql(query, connection)\n",
    "\n",
    "query_ans = \"\"\"SELECT va.hash_id, va.contents, va.question_id FROM viblo_answer va\"\"\"\n",
    "df_answer = pd.read_sql(query_ans, connection)\n",
    "\n",
    "# Close the database connection\n",
    "connection.close()\n",
    "\n",
    "\n",
    "def filter_slug(x):\n",
    "    if x is None:\n",
    "        return ''\n",
    "    t = str(x).replace(\"b'\", \"\").replace(\"'\", \"\")\n",
    "    t1 = json.loads(t)\n",
    "\n",
    "    return t1\n",
    "df_ques['slug_filter'] = df_ques['slug'].apply(filter_slug)\n",
    "print(df_ques['slug_filter'][0])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100          [java]\n",
      "101         [mysql]\n",
      "102          [java]\n",
      "103       [general]\n",
      "104          [java]\n",
      "105       [general]\n",
      "106       [general]\n",
      "107          [java]\n",
      "108    [javascript]\n",
      "109    [javascript]\n",
      "Name: slug_filter, dtype: object\n",
      "y_test [0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n",
      "t [('java',)]\n",
      "class ['agile' 'ai' 'amazon-web-services' 'android-os' 'angularjs'\n",
      " 'artificial-intelligence' 'asp-net' 'attack-techniques' 'aws-lambda'\n",
      " 'backend-development' 'big-data' 'business-analyst' 'c' 'c-2' 'c-lang'\n",
      " 'chatbot' 'clean-code' 'cloud-computing' 'cloud-security'\n",
      " 'computer-network' 'computer-vision' 'content-creator' 'cryptography'\n",
      " 'css' 'cyber-security' 'dart' 'data-science'\n",
      " 'data-structures-and-algorithms' 'database' 'deep-learning'\n",
      " 'design-pattern' 'devops' 'distribute-system' 'docker'\n",
      " 'external-communication' 'flutter' 'forensics' 'freelancer'\n",
      " 'frontend-development' 'general' 'generative-models' 'git' 'golang'\n",
      " 'google-cloud-platform' 'html' 'image-classification'\n",
      " 'image-segmentation' 'infrastructure' 'internal-communication' 'ios'\n",
      " 'java' 'javascript' 'jquery' 'kubernetes' 'laravel' 'linux'\n",
      " 'malware-analysis' 'marketing' 'mathematics' 'ml' 'mlops'\n",
      " 'mobile-development' 'mobile-security' 'mysql' 'net' 'network-security'\n",
      " 'node-js' 'non-tech-job' 'nosql' 'nuxt-js' 'object-detection'\n",
      " 'object-oriented-programming' 'operating-system'\n",
      " 'optimization-algorithms' 'php' 'postgresql' 'programming'\n",
      " 'project-management' 'python' 'rails' 'react-js' 'react-native'\n",
      " 'recommendation-system' 'redis' 'reinforcement-learning'\n",
      " 'risk-management' 'ruby' 'secure-coding' 'security-management'\n",
      " 'security-tools' 'semi-supervised-learning' 'soft-skills'\n",
      " 'software-development' 'source-code' 'speech-processing' 'spring'\n",
      " 'spring-boot' 'sql' 'supervised-learning' 'swift' 'testing' 'typescript'\n",
      " 'unsupervised-learning' 'vue-js' 'vulnerability' 'web-development'\n",
      " 'web-security' 'windows' 'wordpress']\n",
      "3617\n"
     ]
    }
   ],
   "source": [
    "\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import MultiLabelBinarizer\n",
    "\n",
    "Y = df_ques['slug_filter']\n",
    "print(Y[100:110])\n",
    "\n",
    "mlb = MultiLabelBinarizer()\n",
    "Y = mlb.fit_transform(Y)\n",
    "\n",
    "\n",
    "print('y_test', Y[100])\n",
    "t = mlb.inverse_transform(Y[100:101])\n",
    "print('t', t)\n",
    "print('class', mlb.classes_)\n",
    "   \n",
    "X = df_ques['question_content']\n",
    "print(len(X))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Build word2vec for sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from underthesea import word_tokenize\n",
    "\n",
    "def segmentation(text):\n",
    "    try:\n",
    "        return word_tokenize(text, format=\"text\")\n",
    "    except (TypeError, AttributeError):\n",
    "        return ''\n",
    "\n",
    "def split_words(post):\n",
    "    texts = segmentation(post)\n",
    "    try:\n",
    "        t = [str(x.strip('0123456789%@$.,=+-!;/()*\"&^:#|\\n\\t\\' ').lower()) for x in texts.split()]\n",
    "        filtered_list = [item for item in t if item != \"\"]\n",
    "        return filtered_list\n",
    "    except TypeError:\n",
    "        return []\n",
    "    \n",
    "X_1 = X.apply(split_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0    [thế, nào, là, abstract, syntax, tree, ?, so, ...\n",
      "1    [nếu, các, kỹ_thuật, thường, được, sử_dụng, để...\n",
      "Name: question_content, dtype: object\n"
     ]
    }
   ],
   "source": [
    "print(X_1[:2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "sentences = X_1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.models import Word2Vec\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "#To proprely work with scikit's vectorizer\n",
    "merged_questions = [' '.join(question) for question in sentences]\n",
    "document_names = ['Doc {:d}'.format(i) for i in range(len(merged_questions))]\n",
    "\n",
    "def get_tfidf(docs, ngram_range=(1,1), index=None):\n",
    "    vect = TfidfVectorizer(ngram_range=ngram_range)\n",
    "    tfidf = vect.fit_transform(docs).todense()\n",
    "    return pd.DataFrame(tfidf, columns=vect.get_feature_names_out(), index=index).T\n",
    "\n",
    "tfidf = get_tfidf(merged_questions, ngram_range=(1,1), index=document_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build the word2vec model\n",
    "n = 100\n",
    "model = Word2Vec(X_1.values.tolist(), vector_size=n, min_count=1, window = 8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('khác', 20), ('cách', 21), ('như', 22), ('khi', 23), ('cho', 24), ('có_thể', 25), ('với', 26), ('nhau', 27), ('những', 28), ('giải_thích', 29), ('nêu', 30), ('`', 31), ('khác_biệt', 32), ('tại_sao', 33), ('hãy', 34)]\n"
     ]
    }
   ],
   "source": [
    "# Convert the dictionary to a list of key-value pairs and slice the list\n",
    "first_five = list(model.wv.key_to_index.items())[20:35]\n",
    "\n",
    "# Print the first 5 elements\n",
    "print(first_five)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def get_sent_embs(emb_model, sentences = [], n = 50):\n",
    "    sent_embs = []\n",
    "    for desc in range(len(sentences)):\n",
    "        sent_emb = np.zeros((1, n))\n",
    "        if len(sentences[desc]) > 0:\n",
    "            sent_emb = np.zeros((1, n))\n",
    "            div = 0\n",
    "            model = emb_model\n",
    "            for word in sentences[desc]:\n",
    "                if word in model.wv.key_to_index and word in tfidf.index:\n",
    "                    word_emb = model.wv[word]\n",
    "                    weight = tfidf.loc[word, 'Doc {:d}'.format(desc)]\n",
    "                    sent_emb = np.add(sent_emb, word_emb * weight)\n",
    "                    div += weight\n",
    "                else:\n",
    "                    div += 1e-13 #to avoid dividing by 0\n",
    "        if div == 0:\n",
    "            print(desc)\n",
    "\n",
    "        sent_emb = np.divide(sent_emb, div)\n",
    "        sent_embs.append(sent_emb.flatten())\n",
    "    return sent_embs\n",
    "\n",
    "ft_sent = get_sent_embs(emb_model = model, sentences=sentences, n = n)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100\n"
     ]
    }
   ],
   "source": [
    "print(len(ft_sent[1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<pandas.core.indexing._LocIndexer object at 0x2acd4f1a0>\n"
     ]
    }
   ],
   "source": [
    "print(tfidf.loc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def encode(texts = []):\n",
    "    embs = get_sent_embs(emb_model = model, sentences=texts, n = 100)\n",
    "    return embs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.multioutput import MultiOutputClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X_list = ft_sent\n",
    "# Split the data into training and testing sets\n",
    "X_train, X_test, Y_train, Y_test = train_test_split(X_list, Y, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2893 100 2893 109\n"
     ]
    }
   ],
   "source": [
    "print(len(X_train), len(X_train[0]), len(Y_train), len(Y_train[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>#sk-container-id-1 {color: black;background-color: white;}#sk-container-id-1 pre{padding: 0;}#sk-container-id-1 div.sk-toggleable {background-color: white;}#sk-container-id-1 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-1 label.sk-toggleable__label-arrow:before {content: \"▸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-1 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-1 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-1 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-1 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-1 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-1 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"▾\";}#sk-container-id-1 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-1 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-1 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-1 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-1 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-1 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-1 div.sk-item {position: relative;z-index: 1;}#sk-container-id-1 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-1 div.sk-item::before, #sk-container-id-1 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-1 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-1 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-1 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-1 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-1 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-1 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-1 div.sk-label-container {text-align: center;}#sk-container-id-1 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-1 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-1\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>MultiOutputClassifier(estimator=RandomForestClassifier(random_state=42),\n",
       "                      n_jobs=-1)</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item sk-dashed-wrapped\"><div class=\"sk-label-container\"><div class=\"sk-label sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-1\" type=\"checkbox\" ><label for=\"sk-estimator-id-1\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">MultiOutputClassifier</label><div class=\"sk-toggleable__content\"><pre>MultiOutputClassifier(estimator=RandomForestClassifier(random_state=42),\n",
       "                      n_jobs=-1)</pre></div></div></div><div class=\"sk-parallel\"><div class=\"sk-parallel-item\"><div class=\"sk-item\"><div class=\"sk-label-container\"><div class=\"sk-label sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-2\" type=\"checkbox\" ><label for=\"sk-estimator-id-2\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">estimator: RandomForestClassifier</label><div class=\"sk-toggleable__content\"><pre>RandomForestClassifier(random_state=42)</pre></div></div></div><div class=\"sk-serial\"><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-3\" type=\"checkbox\" ><label for=\"sk-estimator-id-3\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">RandomForestClassifier</label><div class=\"sk-toggleable__content\"><pre>RandomForestClassifier(random_state=42)</pre></div></div></div></div></div></div></div></div></div></div>"
      ],
      "text/plain": [
       "MultiOutputClassifier(estimator=RandomForestClassifier(random_state=42),\n",
       "                      n_jobs=-1)"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.svm import SVC\n",
    "# Create a multi-output classifier with a random forest classifier\n",
    "rf = RandomForestClassifier(n_estimators=100, random_state=42)\n",
    "# svc = SVC(kernel='rbf')\n",
    "clf = MultiOutputClassifier(rf, n_jobs=-1)\n",
    "\n",
    "# Fit the classifier to the training data\n",
    "clf.fit(X_train, Y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hamming Loss:  0.01\n",
      "F1 Score:  0.025284450063211124\n",
      "F1 Score:  [0.         0.         0.         0.         0.         0.05128205\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.08510638 0.         0.         0.23529412 0.125      0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.5        0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.18181818 0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.05882353 0.         0.         0.         0.\n",
      " 0.        ]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/ngocp/.pyenv/versions/3.10.3/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1609: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no true nor predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, \"true nor predicted\", \"F-score is\", len(true_sum))\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import hamming_loss, f1_score\n",
    "\n",
    "prediction = clf.predict(X_test)\n",
    "# print('Prediction: ', prediction[4:12])\n",
    "print('Hamming Loss: ', round(hamming_loss(Y_test, prediction),2))\n",
    "print('F1 Score: ', f1_score(Y_test, prediction, average='micro'))\n",
    "\n",
    "print('F1 Score: ', f1_score(Y_test, prediction, average=None))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "leng : Y_pred_free_class 15\n",
      "Phân biệt mạng Object Detection 2 stage và 1 stage -> ('mysql', 'programming')\n",
      "Những lỗ hổng thường gặp với API -> ('mysql', 'programming')\n",
      "IAM là gì, IAM có vai trò gì trong việc đảm bảo an toàn hệ thống AWS -> ('mysql', 'programming')\n",
      "Có bao nhiêu loại lỗi sai trong PHP? -> ('mysql', 'programming')\n",
      "Git Submodule trong trường hợp nào ? -> ('mysql', 'programming')\n",
      "Có sự thừa kế theo cấp bậc giữa các module trong không? Giải thích. -> ('mysql', 'programming')\n",
      "Sự khác biệt giữa từ khóa break và continue trong Java? -> ('mysql', 'programming')\n",
      "Trình bày về Output Buffering trong PHP? -> ('mysql', 'programming')\n",
      "RESTful API là gì? -> ('mysql', 'programming')\n",
      "Viết tắt của php có nghĩa là gì ? -> ('mysql', 'programming')\n",
      "Phân biệt POST và GET trong php? -> ('mysql', 'programming')\n",
      "Cờ HttpOnly có tác dụng gì cho cookie? -> ('mysql', 'programming')\n",
      "so sánh sự khác nhau giữa Mysql và MongoDB -> ('mysql', 'programming')\n",
      "Tại sao phải sử dụng hàm khởi tạo? -> ('mysql', 'programming')\n",
      "Hash trong Ruby có thể lưu trữ các loại dữ liệu nào? -> ('mysql', 'programming')\n"
     ]
    }
   ],
   "source": [
    "X_test_free = [\n",
    "    'Phân biệt mạng Object Detection 2 stage và 1 stage',\n",
    "    'Những lỗ hổng thường gặp với API',\n",
    "    'IAM là gì, IAM có vai trò gì trong việc đảm bảo an toàn hệ thống AWS',\n",
    "    'Có bao nhiêu loại lỗi sai trong PHP?',\n",
    "    'Git Submodule trong trường hợp nào ?',\n",
    "    'Có sự thừa kế theo cấp bậc giữa các module trong không? Giải thích.',\n",
    "    'Sự khác biệt giữa từ khóa break và continue trong Java?',\n",
    "    'Trình bày về Output Buffering trong PHP?',\n",
    "    'RESTful API là gì?',\n",
    "    'Viết tắt của php có nghĩa là gì ?',\n",
    "    'Phân biệt POST và GET trong php?',\n",
    "    'Cờ HttpOnly có tác dụng gì cho cookie?',\n",
    "    'so sánh sự khác nhau giữa Mysql và MongoDB',\n",
    "    'Tại sao phải sử dụng hàm khởi tạo?',\n",
    "    'Hash trong Ruby có thể lưu trữ các loại dữ liệu nào?',\n",
    "]\n",
    "\n",
    "\n",
    "X_test_embed = encode(X_test_free)\n",
    "\n",
    "# Predict the labels for the test data\n",
    "Y_pred_free = clf.predict(X_test_embed)\n",
    "\n",
    "Y_pred_free_class = mlb.inverse_transform(Y_pred_free)\n",
    "print('leng : Y_pred_free_class', len(Y_pred_free_class))\n",
    "for index, tags in enumerate(Y_pred_free_class):\n",
    "    print(X_test_free[index][:100], '->', tags)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
