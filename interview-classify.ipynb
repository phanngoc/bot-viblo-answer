{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/g6/37kt02914kx36yzcbbqfyck00000gn/T/ipykernel_10966/3877717129.py:15: UserWarning: pandas only supports SQLAlchemy connectable (engine/connection) or database string URI or sqlite3 DBAPI2 connection. Other DBAPI2 objects are not tested. Please consider using SQLAlchemy.\n",
      "  df = pd.read_sql(query, connection)\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
    "import mysql.connector\n",
    "\n",
    "# Establish a connection to the MySQL database\n",
    "connection = mysql.connector.connect(\n",
    "    host='127.0.0.1',\n",
    "    port=13306,\n",
    "    user='root',\n",
    "    password='root',\n",
    "    database='pyml'\n",
    ")\n",
    "\n",
    "# Read the table data using pandas\n",
    "query = \"SELECT contents, JSON_UNQUOTE(JSON_EXTRACT(tags, '$[0].slug')) AS slug FROM viblo_interview\"\n",
    "df = pd.read_sql(query, connection)\n",
    "\n",
    "# Close the database connection\n",
    "connection.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "frontend-development\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()\n",
    "df['slug'] = df['slug'].apply(lambda x: str(x).replace(\"b'\", \"\").replace(\"'\", \"\"))\n",
    "print(df['slug'][0])\n",
    "str(df['slug'][0]) == 'frontend-development'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "num_classes 104\n",
      "num_words: 3701\n",
      "Model: \"model\"\n",
      "__________________________________________________________________________________________________\n",
      " Layer (type)                   Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      " inputs (InputLayer)            [(None, 100)]        0           []                               \n",
      "                                                                                                  \n",
      " embedding (Embedding)          (None, 100, 400)     1480400     ['inputs[0][0]']                 \n",
      "                                                                                                  \n",
      " lstm (LSTM)                    (None, 100, 64)      119040      ['embedding[0][0]']              \n",
      "                                                                                                  \n",
      " bidirectional (Bidirectional)  (None, 100, 128)     66048       ['lstm[0][0]']                   \n",
      "                                                                                                  \n",
      " attention (Attention)          (None, 100, 128)     0           ['bidirectional[0][0]',          \n",
      "                                                                  'bidirectional[0][0]']          \n",
      "                                                                                                  \n",
      " max_pooling1d (MaxPooling1D)   (None, 50, 128)      0           ['attention[0][0]']              \n",
      "                                                                                                  \n",
      " dense (Dense)                  (None, 50, 104)      13416       ['max_pooling1d[0][0]']          \n",
      "                                                                                                  \n",
      "==================================================================================================\n",
      "Total params: 1,678,904\n",
      "Trainable params: 1,678,904\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n",
      "Epoch 1/10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-07-01 15:12:38.533786: W tensorflow/tsl/platform/profile_utils/cpu_utils.cc:128] Failed to get CPU frequency: 0 Hz\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "in user code:\n\n    File \"/Users/ngocp/.pyenv/versions/3.10.3/lib/python3.10/site-packages/keras/engine/training.py\", line 1249, in train_function  *\n        return step_function(self, iterator)\n    File \"/Users/ngocp/.pyenv/versions/3.10.3/lib/python3.10/site-packages/keras/engine/training.py\", line 1233, in step_function  **\n        outputs = model.distribute_strategy.run(run_step, args=(data,))\n    File \"/Users/ngocp/.pyenv/versions/3.10.3/lib/python3.10/site-packages/keras/engine/training.py\", line 1222, in run_step  **\n        outputs = model.train_step(data)\n    File \"/Users/ngocp/.pyenv/versions/3.10.3/lib/python3.10/site-packages/keras/engine/training.py\", line 1024, in train_step\n        loss = self.compute_loss(x, y, y_pred, sample_weight)\n    File \"/Users/ngocp/.pyenv/versions/3.10.3/lib/python3.10/site-packages/keras/engine/training.py\", line 1082, in compute_loss\n        return self.compiled_loss(\n    File \"/Users/ngocp/.pyenv/versions/3.10.3/lib/python3.10/site-packages/keras/engine/compile_utils.py\", line 265, in __call__\n        loss_value = loss_obj(y_t, y_p, sample_weight=sw)\n    File \"/Users/ngocp/.pyenv/versions/3.10.3/lib/python3.10/site-packages/keras/losses.py\", line 152, in __call__\n        losses = call_fn(y_true, y_pred)\n    File \"/Users/ngocp/.pyenv/versions/3.10.3/lib/python3.10/site-packages/keras/losses.py\", line 284, in call  **\n        return ag_fn(y_true, y_pred, **self._fn_kwargs)\n    File \"/Users/ngocp/.pyenv/versions/3.10.3/lib/python3.10/site-packages/keras/losses.py\", line 2176, in binary_crossentropy\n        backend.binary_crossentropy(y_true, y_pred, from_logits=from_logits),\n    File \"/Users/ngocp/.pyenv/versions/3.10.3/lib/python3.10/site-packages/keras/backend.py\", line 5680, in binary_crossentropy\n        return tf.nn.sigmoid_cross_entropy_with_logits(\n\n    ValueError: `logits` and `labels` must have the same shape, received ((None, 50, 104) vs (None, 104)).\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[3], line 158\u001b[0m\n\u001b[1;32m    154\u001b[0m         \u001b[39mreturn\u001b[39;00m np\u001b[39m.\u001b[39mvectorize(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mflipped_label_mapping\u001b[39m.\u001b[39mget)(max_indices)\n\u001b[1;32m    157\u001b[0m model \u001b[39m=\u001b[39m ModelBuild()\n\u001b[0;32m--> 158\u001b[0m model\u001b[39m.\u001b[39;49mbuild_model_seq_word2vec(df[\u001b[39m'\u001b[39;49m\u001b[39mcontents\u001b[39;49m\u001b[39m'\u001b[39;49m], df[\u001b[39m'\u001b[39;49m\u001b[39mslug\u001b[39;49m\u001b[39m'\u001b[39;49m])\n",
      "Cell \u001b[0;32mIn[3], line 142\u001b[0m, in \u001b[0;36mModelBuild.build_model_seq_word2vec\u001b[0;34m(self, X_text, y_class)\u001b[0m\n\u001b[1;32m    140\u001b[0m model\u001b[39m.\u001b[39mcompile(optimizer\u001b[39m=\u001b[39m\u001b[39m'\u001b[39m\u001b[39madam\u001b[39m\u001b[39m'\u001b[39m, loss\u001b[39m=\u001b[39m\u001b[39m'\u001b[39m\u001b[39mbinary_crossentropy\u001b[39m\u001b[39m'\u001b[39m, metrics\u001b[39m=\u001b[39m[\u001b[39m'\u001b[39m\u001b[39macc\u001b[39m\u001b[39m'\u001b[39m])\n\u001b[1;32m    141\u001b[0m model\u001b[39m.\u001b[39msummary()\n\u001b[0;32m--> 142\u001b[0m model\u001b[39m.\u001b[39;49mfit(X, y_train, epochs\u001b[39m=\u001b[39;49m\u001b[39m10\u001b[39;49m, batch_size\u001b[39m=\u001b[39;49m\u001b[39m32\u001b[39;49m, validation_split\u001b[39m=\u001b[39;49m\u001b[39m0.2\u001b[39;49m)\n\u001b[1;32m    143\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mmodel \u001b[39m=\u001b[39m model\n\u001b[1;32m    145\u001b[0m \u001b[39mreturn\u001b[39;00m model\n",
      "File \u001b[0;32m~/.pyenv/versions/3.10.3/lib/python3.10/site-packages/keras/utils/traceback_utils.py:70\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     67\u001b[0m     filtered_tb \u001b[39m=\u001b[39m _process_traceback_frames(e\u001b[39m.\u001b[39m__traceback__)\n\u001b[1;32m     68\u001b[0m     \u001b[39m# To get the full stack trace, call:\u001b[39;00m\n\u001b[1;32m     69\u001b[0m     \u001b[39m# `tf.debugging.disable_traceback_filtering()`\u001b[39;00m\n\u001b[0;32m---> 70\u001b[0m     \u001b[39mraise\u001b[39;00m e\u001b[39m.\u001b[39mwith_traceback(filtered_tb) \u001b[39mfrom\u001b[39;00m \u001b[39mNone\u001b[39m\n\u001b[1;32m     71\u001b[0m \u001b[39mfinally\u001b[39;00m:\n\u001b[1;32m     72\u001b[0m     \u001b[39mdel\u001b[39;00m filtered_tb\n",
      "File \u001b[0;32m/var/folders/g6/37kt02914kx36yzcbbqfyck00000gn/T/__autograph_generated_filej91asfih.py:15\u001b[0m, in \u001b[0;36mouter_factory.<locals>.inner_factory.<locals>.tf__train_function\u001b[0;34m(iterator)\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m     14\u001b[0m     do_return \u001b[39m=\u001b[39m \u001b[39mTrue\u001b[39;00m\n\u001b[0;32m---> 15\u001b[0m     retval_ \u001b[39m=\u001b[39m ag__\u001b[39m.\u001b[39mconverted_call(ag__\u001b[39m.\u001b[39mld(step_function), (ag__\u001b[39m.\u001b[39mld(\u001b[39mself\u001b[39m), ag__\u001b[39m.\u001b[39mld(iterator)), \u001b[39mNone\u001b[39;00m, fscope)\n\u001b[1;32m     16\u001b[0m \u001b[39mexcept\u001b[39;00m:\n\u001b[1;32m     17\u001b[0m     do_return \u001b[39m=\u001b[39m \u001b[39mFalse\u001b[39;00m\n",
      "\u001b[0;31mValueError\u001b[0m: in user code:\n\n    File \"/Users/ngocp/.pyenv/versions/3.10.3/lib/python3.10/site-packages/keras/engine/training.py\", line 1249, in train_function  *\n        return step_function(self, iterator)\n    File \"/Users/ngocp/.pyenv/versions/3.10.3/lib/python3.10/site-packages/keras/engine/training.py\", line 1233, in step_function  **\n        outputs = model.distribute_strategy.run(run_step, args=(data,))\n    File \"/Users/ngocp/.pyenv/versions/3.10.3/lib/python3.10/site-packages/keras/engine/training.py\", line 1222, in run_step  **\n        outputs = model.train_step(data)\n    File \"/Users/ngocp/.pyenv/versions/3.10.3/lib/python3.10/site-packages/keras/engine/training.py\", line 1024, in train_step\n        loss = self.compute_loss(x, y, y_pred, sample_weight)\n    File \"/Users/ngocp/.pyenv/versions/3.10.3/lib/python3.10/site-packages/keras/engine/training.py\", line 1082, in compute_loss\n        return self.compiled_loss(\n    File \"/Users/ngocp/.pyenv/versions/3.10.3/lib/python3.10/site-packages/keras/engine/compile_utils.py\", line 265, in __call__\n        loss_value = loss_obj(y_t, y_p, sample_weight=sw)\n    File \"/Users/ngocp/.pyenv/versions/3.10.3/lib/python3.10/site-packages/keras/losses.py\", line 152, in __call__\n        losses = call_fn(y_true, y_pred)\n    File \"/Users/ngocp/.pyenv/versions/3.10.3/lib/python3.10/site-packages/keras/losses.py\", line 284, in call  **\n        return ag_fn(y_true, y_pred, **self._fn_kwargs)\n    File \"/Users/ngocp/.pyenv/versions/3.10.3/lib/python3.10/site-packages/keras/losses.py\", line 2176, in binary_crossentropy\n        backend.binary_crossentropy(y_true, y_pred, from_logits=from_logits),\n    File \"/Users/ngocp/.pyenv/versions/3.10.3/lib/python3.10/site-packages/keras/backend.py\", line 5680, in binary_crossentropy\n        return tf.nn.sigmoid_cross_entropy_with_logits(\n\n    ValueError: `logits` and `labels` must have the same shape, received ((None, 50, 104) vs (None, 104)).\n"
     ]
    }
   ],
   "source": [
    "from keras.models import Sequential\n",
    "from keras import backend as K, initializers, regularizers, constraints, Model\n",
    "from keras.layers import Embedding, Flatten, MaxPooling1D, Dense, LSTM, Bidirectional, Attention, Layer, Input, Activation, Dropout, SpatialDropout1D\n",
    "from keras_preprocessing.text import Tokenizer\n",
    "from keras_preprocessing import sequence\n",
    "from gensim import corpora, models\n",
    "from gensim.models import KeyedVectors\n",
    "import numpy as np\n",
    "from keras.utils import to_categorical\n",
    "import tensorflow as tf\n",
    "\n",
    "# Add attention layer to the deep learning network\n",
    "class attention(Layer):\n",
    "    def __init__(self,**kwargs):\n",
    "        super(attention,self).__init__(**kwargs)\n",
    "\n",
    "    def build(self,input_shape):\n",
    "        self.W=self.add_weight(name='attention_weight', shape=(input_shape[-1],1), \n",
    "                               initializer='random_normal', trainable=True)\n",
    "        self.b=self.add_weight(name='attention_bias', shape=(input_shape[1],1), \n",
    "                               initializer='zeros', trainable=True)        \n",
    "        super(attention, self).build(input_shape)\n",
    "\n",
    "    def call(self,x):\n",
    "        print('current stack:', x.shape, self.W.shape, self.b.shape)\n",
    "        # Alignment scores. Pass them through tanh function\n",
    "        e = K.tanh(K.dot(x,self.W)+self.b)\n",
    "        # Remove dimension of size 1\n",
    "        e = K.squeeze(e, axis=-1)\n",
    "        print('call: e', e.shape)\n",
    "        # Compute the weights\n",
    "        alpha = K.softmax(e)\n",
    "        print('call: alpha', alpha.shape)\n",
    "        # Reshape to tensorFlow format\n",
    "        alpha = K.expand_dims(alpha, axis=-1)\n",
    "        print('call: alpha expand_dimp', alpha.shape)\n",
    "        # Compute the context vector\n",
    "        context = x * alpha\n",
    "        context = K.sum(context, axis=1)\n",
    "        print('call: context', context.shape)\n",
    "        return context\n",
    "    \n",
    "w2v_model = KeyedVectors.load_word2vec_format('../model/wiki.vi.model.bin', binary=True)\n",
    "\n",
    "class ModelBuild:\n",
    "    def __init__(self):\n",
    "        self.tokenizer = Tokenizer()\n",
    "        self.model = None\n",
    "        self.label_mapping = []\n",
    "        self.flipped_label_mapping = {}\n",
    "\n",
    "    def convertToCategories(self, y_class):\n",
    "        # Example list of label texts\n",
    "        labels = y_class\n",
    "\n",
    "        # Create a dictionary to map unique labels to integers\n",
    "        self.label_mapping = {label: i for i, label in enumerate(set(labels))}\n",
    "        self.flipped_label_mapping = {value: key for key, value in self.label_mapping.items()}\n",
    "        # Convert labels to corresponding integers\n",
    "        integer_labels = [self.label_mapping[label] for label in labels]\n",
    "\n",
    "        # One-hot encode the integer labels\n",
    "        encoded_labels = to_categorical(integer_labels)\n",
    "        return encoded_labels\n",
    "\n",
    "    def build_model_word2vec(self, X_text, y_class):        \n",
    "        self.tokenizer.fit_on_texts(X_text)\n",
    "        sequences = self.tokenizer.texts_to_sequences(X_text)\n",
    "\n",
    "        maxlen = 100\n",
    "        X = sequence.pad_sequences(sequences, maxlen=maxlen)\n",
    "\n",
    "        num_classes = y_class.nunique()\n",
    "        y_train = self.convertToCategories(y_class)\n",
    "\n",
    "        print('num_classes', num_classes)\n",
    "        embedding_dim = 400\n",
    "        word_index = self.tokenizer.word_index\n",
    "        num_words = min(len(word_index) + 1, len(w2v_model.index_to_key))\n",
    "        embedding_matrix = np.zeros((num_words, embedding_dim))\n",
    "\n",
    "        print('num_words:', num_words)\n",
    "        for word, i in word_index.items():\n",
    "            if i >= num_words:\n",
    "                continue\n",
    "            if word in w2v_model.index_to_key:\n",
    "                embedding_matrix[i] = w2v_model.get_vector(word)\n",
    "        model = Sequential()\n",
    "        model.add(Embedding(num_words, embedding_dim, input_length=maxlen, weights=[embedding_matrix], trainable=False))\n",
    "        model.add(Bidirectional(LSTM(64, return_sequences=True, input_shape=(maxlen, ))))\n",
    "        # model.add(Flatten())\n",
    "        model.add(attention()) # pass a list of two tensors \n",
    "        model.add(Dense(num_classes, activation='sigmoid'))\n",
    "        model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['acc'])\n",
    "        model.summary()\n",
    "        model.fit(X, y_train, epochs=10, batch_size=32, validation_split=0.2)\n",
    "        self.model = model\n",
    "\n",
    "        return model\n",
    "\n",
    "    def build_model_seq_word2vec(self, X_text, y_class):        \n",
    "        self.tokenizer.fit_on_texts(X_text)\n",
    "        sequences = self.tokenizer.texts_to_sequences(X_text)\n",
    "\n",
    "        maxlen = 100\n",
    "        X = sequence.pad_sequences(sequences, maxlen=maxlen)\n",
    "\n",
    "        num_classes = y_class.nunique()\n",
    "        y_train = self.convertToCategories(y_class)\n",
    "\n",
    "        print('num_classes', num_classes)\n",
    "        embedding_dim = 400\n",
    "        word_index = self.tokenizer.word_index\n",
    "        num_words = min(len(word_index) + 1, len(w2v_model.index_to_key))\n",
    "        embedding_matrix = np.zeros((num_words, embedding_dim))\n",
    "\n",
    "        print('num_words:', num_words)\n",
    "        for word, i in word_index.items():\n",
    "            if i >= num_words:\n",
    "                continue\n",
    "            if word in w2v_model.index_to_key:\n",
    "                embedding_matrix[i] = w2v_model.get_vector(word)\n",
    "\n",
    "        inputs = Input(name='inputs', shape=[maxlen])\n",
    "        layer = Embedding(num_words, embedding_dim, input_length=maxlen)(inputs)\n",
    "        layer = LSTM(64, return_sequences=True)(layer)\n",
    "        layer = Bidirectional(LSTM(64, return_sequences=True))(layer)\n",
    "        layer = Attention()([layer, layer])\n",
    "        # layer = Flatten()(layer)\n",
    "        layer = MaxPooling1D()(layer)\n",
    "        layer = Dense(num_classes, activation='sigmoid')(layer)\n",
    "        # layer = Activation('sigmoid')(layer)\n",
    "        model = Model(inputs=inputs,outputs=layer)\n",
    "        # model = Sequential()\n",
    "        # model.add(Embedding(num_words, embedding_dim, input_length=maxlen, weights=[embedding_matrix], trainable=False))\n",
    "        # model.add(Bidirectional(LSTM(64, return_sequences=True, input_shape=(maxlen, ))))\n",
    "        # # model.add(Flatten())\n",
    "        # model.add(attention()) # pass a list of two tensors \n",
    "        # model.add(Dense(num_classes, activation='sigmoid'))\n",
    "        model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['acc'])\n",
    "        model.summary()\n",
    "        model.fit(X, y_train, epochs=10, batch_size=32, validation_split=0.2)\n",
    "        self.model = model\n",
    "\n",
    "        return model\n",
    "\n",
    "    def predict(self, X_text):\n",
    "        sequences = self.tokenizer.texts_to_sequences(X_text)\n",
    "        maxlen = 100\n",
    "        X = sequence.pad_sequences(sequences, maxlen=maxlen)\n",
    "        t = self.model.predict(X)\n",
    "        max_indices = np.argmax(t, axis=1)\n",
    "        print('predict', max_indices, self.flipped_label_mapping)\n",
    "        return np.vectorize(self.flipped_label_mapping.get)(max_indices)\n",
    "\n",
    "\n",
    "model = ModelBuild()\n",
    "model.build_model_seq_word2vec(df['contents'], df['slug'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 249ms/step\n",
      "predict [21 10 37 21 71 10 71 21 71 21] {0: 'asp-net', 1: 'laravel', 2: 'content-creator', 3: 'data-structures-and-algorithms', 4: 'android-os', 5: 'ai', 6: 'data-science', 7: 'typescript', 8: 'c-2', 9: 'ml', 10: 'ruby', 11: 'rails', 12: 'devops', 13: 'python', 14: 'design-pattern', 15: 'None', 16: 'frontend-development', 17: 'google-cloud-platform', 18: 'jquery', 19: 'mobile-development', 20: 'programming', 21: 'git', 22: 'wordpress', 23: 'java', 24: 'non-tech-jo', 25: 'css', 26: 'chatbot', 27: 'computer-network', 28: 'external-communication', 29: 'nosql', 30: 'artificial-intelligence', 31: 'postgresql', 32: 'nuxt-js', 33: 'dart', 34: 'malware-analysis', 35: 'web-security', 36: 'network-security', 37: 'net', 38: 'security-tools', 39: 'react-native', 40: 'secure-coding', 41: 'redis', 42: 'risk-management', 43: 'source-code', 44: 'cyber-security', 45: 'golang', 46: 'linux', 47: 'agile', 48: 'mysql', 49: 'cloud-security', 50: 'marketing', 51: 'business-analyst', 52: 'project-management', 53: 'general', 54: 'security-management', 55: 'sql', 56: 'object-oriented-programming', 57: 'infrastructure', 58: 'cryptography', 59: 'php', 60: 'docker', 61: 'distribute-system', 62: 'swift', 63: 'software-development', 64: 'testing', 65: 'html', 66: 'web-development', 67: 'flutter', 68: 'cloud-computing', 69: 'c', 70: 'internal-communication', 71: 'backend-development', 72: 'node-js', 73: 'javascript', 74: 'kubernetes', 75: 'clean-code', 76: 'vue-js', 77: 'c-lang', 78: 'attack-techniques', 79: 'amazon-web-services', 80: 'react-js', 81: 'database', 82: 'soft-skills', 83: 'mathematics', 84: 'mobile-security', 85: 'operating-system', 86: 'big-data', 87: 'image-classification'}\n",
      "['git' 'ruby' 'net' 'git' 'backend-development' 'ruby'\n",
      " 'backend-development' 'git' 'backend-development' 'git']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-06-04 18:19:10.312372: W tensorflow/core/grappler/costs/op_level_cost_estimator.cc:690] Error in PredictCost() for the op: op: \"Softmax\" attr { key: \"T\" value { type: DT_FLOAT } } inputs { dtype: DT_FLOAT shape { unknown_rank: true } } device { type: \"CPU\" model: \"0\" num_cores: 8 environment { key: \"cpu_instruction_set\" value: \"ARM NEON\" } environment { key: \"eigen\" value: \"3.4.90\" } l1_cache_size: 16384 l2_cache_size: 524288 l3_cache_size: 524288 memory_size: 268435456 } outputs { dtype: DT_FLOAT shape { unknown_rank: true } }\n"
     ]
    }
   ],
   "source": [
    "X_test_sentence = [\n",
    "    'Git Submodule trong trường hợp nào ?',\n",
    "    'Có sự thừa kế theo cấp bậc giữa các module trong không? Giải thích.',\n",
    "    'Sự khác biệt giữa từ khóa break và continue trong Java?',\n",
    "    'Trình bày về Output Buffering trong PHP?',\n",
    "    'RESTful API là gì?',\n",
    "    'Viết tắt của php có nghĩa là gì ?',\n",
    "    'Phân biệt POST và GET trong php?',\n",
    "    'Cờ HttpOnly có tác dụng gì cho cookie?',\n",
    "    'so sánh sự khác nhau giữa Mysql và MongoDB',\n",
    "    'Tại sao phải sử dụng hàm khởi tạo?',\n",
    "]\n",
    "\n",
    "results = model.predict(X_test_sentence)\n",
    "print(results)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.3"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
